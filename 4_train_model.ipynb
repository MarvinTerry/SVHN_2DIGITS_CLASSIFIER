{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95684cdb",
   "metadata": {},
   "source": [
    "Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e1289ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-11 18:32:41.500854: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-11 18:32:41.502613: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-11 18:32:41.540569: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-11 18:32:41.541222: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-11 18:32:42.293291: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input, regularizers\n",
    "\n",
    "def conv_net(x):\n",
    "    y = layers.Conv2D(16, (5, 5), activation='relu', padding='SAME')(x)\n",
    "    y = layers.Dropout(0.5)(y)\n",
    "    y = layers.MaxPooling2D((2, 2))(y)\n",
    "    y = layers.Conv2D(32, (5, 5), activation='relu', padding='SAME')(y)\n",
    "    y = layers.Dropout(0.5)(y)\n",
    "    y = layers.MaxPooling2D((2, 2))(y)\n",
    "    y = layers.Conv2D(64, (5, 5), activation='relu', padding='SAME')(y)\n",
    "    y = layers.Dropout(0.5)(y)\n",
    "    y = layers.MaxPooling2D((2, 2))(y)\n",
    "    y = layers.Conv2D(128, (5, 5), activation='relu', padding='SAME')(y)\n",
    "    y = layers.Dropout(0.5)(y)\n",
    "    y = layers.MaxPooling2D((2, 2))(y)\n",
    "    y = layers.Conv2D(256, (5, 5), activation='relu', padding='SAME')(y)\n",
    "    y = layers.Dropout(0.5)(y)\n",
    "    y = layers.MaxPooling2D((2, 2))(y)\n",
    "    y = layers.Flatten()(y)\n",
    "    return y\n",
    "\n",
    "def classifier_net(x):\n",
    "    y = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    y = layers.Dropout(0.5)(y)\n",
    "    y = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001))(y)\n",
    "    y = layers.Dropout(0.5)(y)\n",
    "    y = layers.Dense(10, activation='softmax')(y)\n",
    "    return y\n",
    "\n",
    "def model():\n",
    "    inputs = Input(shape=(32,32,1))\n",
    "    \n",
    "    conv_output = conv_net(inputs)\n",
    "    branches_output = [classifier_net(conv_output) for _ in range(2)] \n",
    "    merged_output = tf.stack(branches_output, axis=1)\n",
    "    \n",
    "    # 创建多输出模型\n",
    "    model = models.Model(inputs=inputs, outputs=merged_output)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f54bd50",
   "metadata": {},
   "source": [
    "batch data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6feee572-a287-4de8-8dc1-8f8b2e2df7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def get_steps_count(h5_dataset_path, batch_size):\n",
    "    with h5py.File(h5_dataset_path, 'r') as f:\n",
    "        num_samples = f['images'].shape[0]\n",
    "        num_batches = int(np.ceil(num_samples / batch_size))\n",
    "    return num_batches\n",
    "\n",
    "def data_generator(h5_dataset_path, batch_size):\n",
    "    with h5py.File(h5_dataset_path, 'r') as f:\n",
    "        num_samples = f['images'].shape[0]\n",
    "        num_batches = int(np.ceil(num_samples / batch_size))\n",
    "        \n",
    "        while True:  # 生成器需要无限循环\n",
    "            index_list = list(range(num_batches))\n",
    "            random.shuffle(index_list)\n",
    "            for i in index_list:\n",
    "                start_index = i * batch_size\n",
    "                end_index = min((i + 1) * batch_size, num_samples)\n",
    "\n",
    "                batch_images = f['images'][start_index:end_index]\n",
    "                batch_labels = f['labels'][start_index:end_index]\n",
    "                \n",
    "                yield batch_images, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6574e60f",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58b76cf0-4f36-4906-9dc8-f4bb6022c721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "567/567 [==============================] - 14s 22ms/step - loss: 2.7878 - accuracy: 0.1542 - val_loss: 2.4816 - val_accuracy: 0.1839 - lr: 3.0000e-04\n",
      "Epoch 2/60\n",
      "567/567 [==============================] - 13s 23ms/step - loss: 2.3250 - accuracy: 0.1819 - val_loss: 2.2684 - val_accuracy: 0.2168 - lr: 3.0000e-04\n",
      "Epoch 3/60\n",
      "567/567 [==============================] - 13s 23ms/step - loss: 2.0351 - accuracy: 0.2726 - val_loss: 2.1424 - val_accuracy: 0.2516 - lr: 3.0000e-04\n",
      "Epoch 4/60\n",
      "567/567 [==============================] - 13s 23ms/step - loss: 1.8505 - accuracy: 0.3179 - val_loss: 2.0448 - val_accuracy: 0.2978 - lr: 3.0000e-04\n",
      "Epoch 5/60\n",
      "567/567 [==============================] - 13s 23ms/step - loss: 1.7087 - accuracy: 0.3668 - val_loss: 1.9465 - val_accuracy: 0.3333 - lr: 3.0000e-04\n",
      "Epoch 6/60\n",
      "567/567 [==============================] - 13s 23ms/step - loss: 1.5806 - accuracy: 0.4175 - val_loss: 1.9030 - val_accuracy: 0.3747 - lr: 3.0000e-04\n",
      "Epoch 7/60\n",
      "567/567 [==============================] - 13s 23ms/step - loss: 1.3990 - accuracy: 0.5060 - val_loss: 1.7480 - val_accuracy: 0.4800 - lr: 3.0000e-04\n",
      "Epoch 8/60\n",
      "567/567 [==============================] - 14s 25ms/step - loss: 1.2095 - accuracy: 0.5965 - val_loss: 1.5955 - val_accuracy: 0.5567 - lr: 3.0000e-04\n",
      "Epoch 9/60\n",
      "567/567 [==============================] - 15s 26ms/step - loss: 1.0741 - accuracy: 0.6508 - val_loss: 1.5375 - val_accuracy: 0.5722 - lr: 3.0000e-04\n",
      "Epoch 10/60\n",
      "567/567 [==============================] - 14s 24ms/step - loss: 0.9812 - accuracy: 0.6835 - val_loss: 1.4161 - val_accuracy: 0.6461 - lr: 3.0000e-04\n",
      "Epoch 11/60\n",
      "567/567 [==============================] - 13s 23ms/step - loss: 0.9165 - accuracy: 0.7090 - val_loss: 1.3790 - val_accuracy: 0.6626 - lr: 3.0000e-04\n",
      "Epoch 12/60\n",
      "567/567 [==============================] - 13s 24ms/step - loss: 0.8597 - accuracy: 0.7322 - val_loss: 1.3260 - val_accuracy: 0.7152 - lr: 3.0000e-04\n",
      "Epoch 13/60\n",
      "567/567 [==============================] - 14s 24ms/step - loss: 0.8159 - accuracy: 0.7455 - val_loss: 1.2615 - val_accuracy: 0.7119 - lr: 3.0000e-04\n",
      "Epoch 14/60\n",
      "567/567 [==============================] - 14s 25ms/step - loss: 0.7851 - accuracy: 0.7593 - val_loss: 1.1863 - val_accuracy: 0.7487 - lr: 3.0000e-04\n",
      "Epoch 15/60\n",
      "567/567 [==============================] - 16s 28ms/step - loss: 0.7503 - accuracy: 0.7732 - val_loss: 1.1935 - val_accuracy: 0.7575 - lr: 3.0000e-04\n",
      "Epoch 16/60\n",
      "567/567 [==============================] - 14s 25ms/step - loss: 0.7243 - accuracy: 0.7808 - val_loss: 1.1479 - val_accuracy: 0.7683 - lr: 3.0000e-04\n",
      "Epoch 17/60\n",
      "567/567 [==============================] - 14s 24ms/step - loss: 0.7031 - accuracy: 0.7868 - val_loss: 1.1988 - val_accuracy: 0.7484 - lr: 3.0000e-04\n",
      "Epoch 18/60\n",
      "567/567 [==============================] - 14s 24ms/step - loss: 0.6836 - accuracy: 0.7983 - val_loss: 1.1259 - val_accuracy: 0.7454 - lr: 3.0000e-04\n",
      "Epoch 19/60\n",
      "567/567 [==============================] - 14s 25ms/step - loss: 0.6751 - accuracy: 0.8003 - val_loss: 1.1444 - val_accuracy: 0.7303 - lr: 3.0000e-04\n",
      "Epoch 20/60\n",
      "567/567 [==============================] - 13s 24ms/step - loss: 0.6588 - accuracy: 0.8048 - val_loss: 1.0482 - val_accuracy: 0.7845 - lr: 3.0000e-04\n",
      "Epoch 21/60\n",
      "567/567 [==============================] - 14s 24ms/step - loss: 0.6345 - accuracy: 0.8134 - val_loss: 1.0734 - val_accuracy: 0.7811 - lr: 3.0000e-04\n",
      "Epoch 22/60\n",
      "567/567 [==============================] - 14s 24ms/step - loss: 0.6265 - accuracy: 0.8169 - val_loss: 1.0496 - val_accuracy: 0.7784 - lr: 3.0000e-04\n",
      "Epoch 23/60\n",
      "567/567 [==============================] - 14s 24ms/step - loss: 0.6169 - accuracy: 0.8223 - val_loss: 1.0027 - val_accuracy: 0.8031 - lr: 3.0000e-04\n",
      "Epoch 24/60\n",
      "567/567 [==============================] - 14s 24ms/step - loss: 0.6011 - accuracy: 0.8277 - val_loss: 0.9544 - val_accuracy: 0.8058 - lr: 3.0000e-04\n",
      "Epoch 25/60\n",
      "567/567 [==============================] - 13s 23ms/step - loss: 0.5986 - accuracy: 0.8279 - val_loss: 0.9969 - val_accuracy: 0.7964 - lr: 3.0000e-04\n",
      "Epoch 26/60\n",
      "567/567 [==============================] - 13s 23ms/step - loss: 0.5797 - accuracy: 0.8341 - val_loss: 0.9768 - val_accuracy: 0.8129 - lr: 3.0000e-04\n",
      "Epoch 27/60\n",
      "567/567 [==============================] - ETA: 0s - loss: 0.5739 - accuracy: 0.8363\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.00021000000997446476.\n",
      "567/567 [==============================] - 13s 22ms/step - loss: 0.5739 - accuracy: 0.8363 - val_loss: 0.9865 - val_accuracy: 0.8131 - lr: 3.0000e-04\n",
      "Epoch 28/60\n",
      "567/567 [==============================] - 13s 23ms/step - loss: 0.5482 - accuracy: 0.8447 - val_loss: 0.9311 - val_accuracy: 0.8182 - lr: 2.1000e-04\n",
      "Epoch 29/60\n",
      "567/567 [==============================] - 13s 23ms/step - loss: 0.5427 - accuracy: 0.8450 - val_loss: 0.9208 - val_accuracy: 0.8299 - lr: 2.1000e-04\n",
      "Epoch 30/60\n",
      "567/567 [==============================] - 12s 22ms/step - loss: 0.5217 - accuracy: 0.8512 - val_loss: 0.9331 - val_accuracy: 0.8165 - lr: 2.1000e-04\n",
      "Epoch 31/60\n",
      "567/567 [==============================] - 13s 22ms/step - loss: 0.5262 - accuracy: 0.8498 - val_loss: 0.9173 - val_accuracy: 0.8161 - lr: 2.1000e-04\n",
      "Epoch 32/60\n",
      "567/567 [==============================] - 13s 22ms/step - loss: 0.5092 - accuracy: 0.8538 - val_loss: 0.8808 - val_accuracy: 0.8265 - lr: 2.1000e-04\n",
      "Epoch 33/60\n",
      "567/567 [==============================] - 13s 22ms/step - loss: 0.5091 - accuracy: 0.8558 - val_loss: 0.8641 - val_accuracy: 0.8271 - lr: 2.1000e-04\n",
      "Epoch 34/60\n",
      "567/567 [==============================] - 13s 22ms/step - loss: 0.4966 - accuracy: 0.8590 - val_loss: 0.9011 - val_accuracy: 0.8194 - lr: 2.1000e-04\n",
      "Epoch 35/60\n",
      "567/567 [==============================] - 12s 22ms/step - loss: 0.4937 - accuracy: 0.8602 - val_loss: 0.8784 - val_accuracy: 0.8204 - lr: 2.1000e-04\n",
      "Epoch 36/60\n",
      "565/567 [============================>.] - ETA: 0s - loss: 0.4990 - accuracy: 0.8594\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.00014700000901939346.\n",
      "567/567 [==============================] - 13s 22ms/step - loss: 0.4989 - accuracy: 0.8594 - val_loss: 0.8807 - val_accuracy: 0.8317 - lr: 2.1000e-04\n",
      "Epoch 37/60\n",
      "567/567 [==============================] - 13s 22ms/step - loss: 0.4796 - accuracy: 0.8644 - val_loss: 0.8346 - val_accuracy: 0.8344 - lr: 1.4700e-04\n",
      "Epoch 38/60\n",
      "567/567 [==============================] - 13s 22ms/step - loss: 0.4677 - accuracy: 0.8677 - val_loss: 0.8471 - val_accuracy: 0.8281 - lr: 1.4700e-04\n",
      "Epoch 39/60\n",
      "567/567 [==============================] - 13s 22ms/step - loss: 0.4697 - accuracy: 0.8669 - val_loss: 0.8467 - val_accuracy: 0.8289 - lr: 1.4700e-04\n",
      "Epoch 40/60\n",
      "567/567 [==============================] - 13s 23ms/step - loss: 0.4566 - accuracy: 0.8708 - val_loss: 0.8224 - val_accuracy: 0.8326 - lr: 1.4700e-04\n",
      "Epoch 41/60\n",
      "567/567 [==============================] - 13s 22ms/step - loss: 0.4572 - accuracy: 0.8707 - val_loss: 0.8170 - val_accuracy: 0.8354 - lr: 1.4700e-04\n",
      "Epoch 42/60\n",
      "567/567 [==============================] - 13s 23ms/step - loss: 0.4531 - accuracy: 0.8708 - val_loss: 0.8113 - val_accuracy: 0.8383 - lr: 1.4700e-04\n",
      "Epoch 43/60\n",
      "567/567 [==============================] - 13s 22ms/step - loss: 0.4498 - accuracy: 0.8721 - val_loss: 0.8105 - val_accuracy: 0.8387 - lr: 1.4700e-04\n",
      "Epoch 44/60\n",
      "567/567 [==============================] - 13s 22ms/step - loss: 0.4523 - accuracy: 0.8720 - val_loss: 0.8410 - val_accuracy: 0.8262 - lr: 1.4700e-04\n",
      "Epoch 45/60\n",
      "567/567 [==============================] - 13s 22ms/step - loss: 0.4410 - accuracy: 0.8756 - val_loss: 0.8023 - val_accuracy: 0.8353 - lr: 1.4700e-04\n",
      "Epoch 46/60\n",
      "567/567 [==============================] - 13s 22ms/step - loss: 0.4420 - accuracy: 0.8750 - val_loss: 0.8117 - val_accuracy: 0.8439 - lr: 1.4700e-04\n",
      "Epoch 47/60\n",
      "567/567 [==============================] - 13s 23ms/step - loss: 0.4466 - accuracy: 0.8729 - val_loss: 0.7963 - val_accuracy: 0.8446 - lr: 1.4700e-04\n",
      "Epoch 48/60\n",
      "567/567 [==============================] - 13s 22ms/step - loss: 0.4352 - accuracy: 0.8767 - val_loss: 0.8303 - val_accuracy: 0.8325 - lr: 1.4700e-04\n",
      "Epoch 49/60\n",
      "567/567 [==============================] - 13s 23ms/step - loss: 0.4345 - accuracy: 0.8765 - val_loss: 0.7894 - val_accuracy: 0.8452 - lr: 1.4700e-04\n",
      "Epoch 50/60\n",
      "567/567 [==============================] - 13s 22ms/step - loss: 0.4359 - accuracy: 0.8764 - val_loss: 0.8121 - val_accuracy: 0.8320 - lr: 1.4700e-04\n",
      "Epoch 51/60\n",
      "567/567 [==============================] - 13s 23ms/step - loss: 0.4309 - accuracy: 0.8761 - val_loss: 0.7669 - val_accuracy: 0.8396 - lr: 1.4700e-04\n",
      "Epoch 52/60\n",
      "567/567 [==============================] - 13s 22ms/step - loss: 0.4293 - accuracy: 0.8785 - val_loss: 0.7649 - val_accuracy: 0.8435 - lr: 1.4700e-04\n",
      "Epoch 53/60\n",
      "567/567 [==============================] - 13s 23ms/step - loss: 0.4294 - accuracy: 0.8780 - val_loss: 0.8095 - val_accuracy: 0.8263 - lr: 1.4700e-04\n",
      "Epoch 54/60\n",
      "567/567 [==============================] - 13s 23ms/step - loss: 0.4263 - accuracy: 0.8799 - val_loss: 0.8034 - val_accuracy: 0.8283 - lr: 1.4700e-04\n",
      "Epoch 55/60\n",
      "566/567 [============================>.] - ETA: 0s - loss: 0.4252 - accuracy: 0.8775\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 0.00010290000936947763.\n",
      "567/567 [==============================] - 13s 23ms/step - loss: 0.4254 - accuracy: 0.8774 - val_loss: 0.7818 - val_accuracy: 0.8424 - lr: 1.4700e-04\n",
      "Epoch 56/60\n",
      "567/567 [==============================] - 13s 23ms/step - loss: 0.4098 - accuracy: 0.8853 - val_loss: 0.7556 - val_accuracy: 0.8480 - lr: 1.0290e-04\n",
      "Epoch 57/60\n",
      "567/567 [==============================] - 13s 23ms/step - loss: 0.4091 - accuracy: 0.8837 - val_loss: 0.7492 - val_accuracy: 0.8489 - lr: 1.0290e-04\n",
      "Epoch 58/60\n",
      "567/567 [==============================] - 13s 23ms/step - loss: 0.4066 - accuracy: 0.8841 - val_loss: 0.7425 - val_accuracy: 0.8457 - lr: 1.0290e-04\n",
      "Epoch 59/60\n",
      "567/567 [==============================] - 13s 23ms/step - loss: 0.4054 - accuracy: 0.8860 - val_loss: 0.7519 - val_accuracy: 0.8453 - lr: 1.0290e-04\n",
      "Epoch 60/60\n",
      "567/567 [==============================] - 13s 23ms/step - loss: 0.4023 - accuracy: 0.8853 - val_loss: 0.7537 - val_accuracy: 0.8333 - lr: 1.0290e-04\n"
     ]
    }
   ],
   "source": [
    "epochs = 60\n",
    "batch_size = 32\n",
    "\n",
    "train_data_generator = data_generator('svhn_dataset/svhn_2digits_train.h5',batch_size)\n",
    "train_steps = get_steps_count('svhn_dataset/svhn_2digits_train.h5',batch_size)\n",
    "test_data_generator = data_generator('svhn_dataset/svhn_2digits_test.h5',batch_size)\n",
    "test_steps = get_steps_count('svhn_dataset/svhn_2digits_test.h5',batch_size)\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.7, \n",
    "    patience=3, \n",
    "    min_lr=1e-6, \n",
    "    verbose=1)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0003, clipvalue=1.0)\n",
    "\n",
    "model = model()\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_data_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_data_generator,\n",
    "    validation_steps=test_steps,\n",
    "    callbacks=[reduce_lr]\n",
    ")\n",
    "\n",
    "model.save('models/svhn_2digits_model.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
